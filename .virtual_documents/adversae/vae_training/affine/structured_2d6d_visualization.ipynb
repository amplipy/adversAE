


import torch
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns

# Explicit imports from modular structure
import structured_2d6d_autoencoder as s2d6d
import affine_autoencoder_shared as shared

print("ğŸ“š All modules loaded successfully!")
print(f"PyTorch version: {torch.__version__}")
print(f"Device available: {torch.cuda.is_available() and 'CUDA' or 'CPU'}")


# ğŸ”§ CONFIGURATION
# Set up basic configuration for visualization
VIZ_CONFIG = {
    'force_cuda': False,  # Set to True if you want GPU
    'batch_size_test': 128,
    'data_dir': '../data',
    'pin_memory': True,
    'num_workers': 4,
    'grid_size': 30,  # For latent space grid scan
    'latent_range': 1.0,  # Range for latent space scanning (-range to +range)
    'figure_dpi': 100,
    'save_plots': False  # Set to True to save plot images
}

# Get device and data loaders
device = shared.get_cloud_device(VIZ_CONFIG) if VIZ_CONFIG['force_cuda'] else torch.device('cpu')
print(f"ğŸ–¥ï¸ Using device: {device}")

# Load test data for visualization
_, test_loader = shared.get_cloud_mnist_loaders(**{k: v for k, v in VIZ_CONFIG.items() 
                                                   if k in ['batch_size_test', 'data_dir', 'pin_memory', 'num_workers']})
print("ğŸ“Š Test data loaded successfully!")


# ğŸ“ LOAD YOUR PRE-TRAINED MODEL
# Update the model_path with your downloaded model file

# Option 1: Load specific model by filename (RECOMMENDED)
model_path = 'structured_2d6d_simplified_20250721_014252.pth'  # Update this path!

try:
    loaded_model, viz_data = shared.load_model_for_viz(
        model_path, 
        s2d6d.StructuredAffineInvariantAutoEncoder, 
        device
    )
    
    # Extract data for convenience
    model = loaded_model
    original_config = viz_data.get('config', {})
    losses_dict = viz_data.get('losses', {})
    extra_data = viz_data.get('extra_data', {})
    
    print(f"âœ… Model loaded successfully!")
    print(f"ğŸ“… Timestamp: {viz_data.get('timestamp', 'Unknown')}")
    print(f"ğŸ—ï¸ Model type: {viz_data.get('model_type', 'Unknown')}")
    print(f"âš™ï¸ Content dim: {original_config.get('content_latent_dim', '?')}")
    print(f"âš™ï¸ Transform dim: {original_config.get('transform_latent_dim', '?')}")
    
    model_loaded = True
    
except FileNotFoundError:
    print(f"âŒ Model file not found: {model_path}")
    print("ğŸ’¡ Please update the model_path variable with the correct path to your .pth file")
    model_loaded = False
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    model_loaded = False


# ğŸ¯ QUICK MODEL SUMMARY
if model_loaded:
    print("ğŸ” MODEL SUMMARY")
    print("="*50)
    
    # Model architecture
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š Total parameters: {total_params:,}")
    print(f"ğŸ›ï¸ Trainable parameters: {trainable_params:,}")
    
    # Training info if available
    if losses_dict:
        if 'total_losses' in losses_dict and losses_dict['total_losses']:
            final_loss = losses_dict['total_losses'][-1]
            print(f"ğŸ“‰ Final training loss: {final_loss:.4f}")
        print(f"ğŸƒ Epochs trained: {len(losses_dict.get('total_losses', []))}")
    
    # Configuration
    print(f"âš™ï¸ Alpha (affine weight): {original_config.get('alpha', '?')}")
    print(f"âš™ï¸ Beta (KL weight): {original_config.get('beta', '?')}")
    print(f"ğŸ¯ Simplified loss function: âœ…")
    
    print("\nâœ… Ready for visualization!")
else:
    print("âŒ Model not loaded. Please fix the model path in the previous cell.")


# ğŸŒŸ LATENT SPACE GRID SCAN - MAIN FEATURE
# This creates a grid of digits by scanning through the 2D content latent space

def create_latent_space_grid(model, device, grid_size=10, latent_range=3.0, transform_latent=None):
    """
    Create a grid of generated digits by scanning the 2D content latent space
    
    Args:
        model: Trained StructuredAffineInvariantAutoEncoder
        device: torch device
        grid_size: Size of the grid (grid_size x grid_size)
        latent_range: Range of latent values to scan (-range to +range)
        transform_latent: Fixed 6D transform latent (None for zeros)
    """
    model.eval()
    
    # Create grid of 2D content latent values
    x_vals = np.linspace(-latent_range, latent_range, grid_size)
    y_vals = np.linspace(-latent_range, latent_range, grid_size)
    
    # Generate images for each grid point
    grid_images = []
    
    with torch.no_grad():
        for i, y in enumerate(y_vals):
            row_images = []
            for j, x in enumerate(x_vals):
                # Create 2D content latent
                content_latent = torch.tensor([[x, y]], dtype=torch.float32, device=device)
                
                # Create 6D transform latent (zeros or custom)
                if transform_latent is None:
                    transform_latent_tensor = torch.zeros(1, 6, device=device)
                else:
                    transform_latent_tensor = torch.tensor([transform_latent], dtype=torch.float32, device=device)
                
                # Combine latents and decode using the structured autoencoder's decoder
                combined_latent = torch.cat([content_latent, transform_latent_tensor], dim=1)
                reconstructed = model.structured_autoencoder.decoder(combined_latent)
                
                # Convert to numpy for plotting
                img = reconstructed.cpu().squeeze().numpy()
                row_images.append(img)
            
            grid_images.append(row_images)
    
    return np.array(grid_images), x_vals, y_vals

if model_loaded:
    print("ğŸ¨ Creating latent space grid scan...")
    
    # Create the grid
    grid_images, x_vals, y_vals = create_latent_space_grid(
        model, device, 
        grid_size=VIZ_CONFIG['grid_size'], 
        latent_range=VIZ_CONFIG['latent_range']
    )
    
    # Plot the grid
    fig, axes = plt.subplots(VIZ_CONFIG['grid_size'], VIZ_CONFIG['grid_size'], 
                           figsize=(12, 12))
    fig.suptitle('ğŸ—ºï¸ 2D Content Latent Space Grid Scan', fontsize=16, y=0.95)
    
    for i in range(VIZ_CONFIG['grid_size']):
        for j in range(VIZ_CONFIG['grid_size']):
            axes[i, j].imshow(grid_images[i, j], cmap='gray')
            axes[i, j].set_xticks([])
            axes[i, j].set_yticks([])
            
            # Add latent coordinates as titles for edge cases
            if i == 0:  # Top row
                axes[i, j].set_title(f'{x_vals[j]:.1f}', fontsize=8, pad=2)
            if j == 0:  # Left column  
                axes[i, j].set_ylabel(f'{y_vals[i]:.1f}', fontsize=8, rotation=0, ha='right')
    
    # Add axis labels
    fig.text(0.5, 0.02, 'Content Latent Dimension 1 â†’', ha='center', fontsize=12)
    fig.text(0.02, 0.5, 'Content Latent Dimension 2 â†’', va='center', rotation=90, fontsize=12)
    
    plt.tight_layout()
    plt.subplots_adjust(top=0.93, bottom=0.07, left=0.07, right=0.95)
    plt.show()
    
    print(f"âœ… Grid scan complete! Generated {VIZ_CONFIG['grid_size']}Ã—{VIZ_CONFIG['grid_size']} = {VIZ_CONFIG['grid_size']**2} images")
    print(f"ğŸ“ Scanned range: [{-VIZ_CONFIG['latent_range']:.1f}, {VIZ_CONFIG['latent_range']:.1f}] in both dimensions")

else:
    print("âŒ Cannot create grid scan without loaded model.")





# ğŸ¨ COMPREHENSIVE VISUALIZATIONS
# Apply all available visualization functions to the loaded model

if model_loaded:
    print("ğŸ¨ Running comprehensive visualizations...")
    
    # 1. Training progress (if available)
    if losses_dict and any(losses_dict.get(k, []) for k in ['total_losses', 'recon_losses', 'kl_losses']):
        print("ğŸ“ˆ Plotting training progress...")
        s2d6d.plot_simplified_training_progress_structured(losses_dict)
    else:
        print("âš ï¸ No training loss data available to plot")
    
    # 2. Latent space visualization
    print("ğŸ—ºï¸ Visualizing latent space...")
    content_data, transform_data, label_data = s2d6d.visualize_structured_latent_space(
        model, test_loader, device
    )
    
    # 3. Comprehensive visualization with config
    print("ğŸ¯ Running comprehensive visualization...")
    # Use original config if available, otherwise use reasonable defaults
    viz_config = original_config if original_config else {
        'alpha': 1.0, 'beta': 0.001, 'content_latent_dim': 2, 'transform_latent_dim': 6
    }
    s2d6d.comprehensive_visualization_structured(model, test_loader, device, viz_config)
    
    print("âœ… All visualizations complete!")
    
else:
    print("âŒ Cannot run visualizations without loaded model.")


# ğŸ”„ TRANSFORM SPACE EXPLORATION
# Show how the 6D transform space affects the same content

def explore_with_transforms(model, device, content_point=[0, 0], num_transforms=8):
    """Show how different 6D transforms affect the same 2D content"""
    model.eval()
    
    fig, axes = plt.subplots(2, 4, figsize=(12, 6))
    axes = axes.flatten()
    
    content_latent = torch.tensor([content_point], dtype=torch.float32, device=device)
    
    with torch.no_grad():
        for i in range(num_transforms):
            # Generate random transform
            transform_latent = torch.randn(1, 6, device=device) * 0.5  # Random transforms
            
            # Generate image using proper decoder access
            combined_latent = torch.cat([content_latent, transform_latent], dim=1)
            reconstructed = model.structured_autoencoder.decoder(combined_latent)
            img = reconstructed.cpu().squeeze().numpy()
            
            axes[i].imshow(img, cmap='gray')
            axes[i].set_title(f'Transform {i+1}', fontsize=10)
            axes[i].set_xticks([])
            axes[i].set_yticks([])
    
    fig.suptitle(f'ğŸ­ Same Content [{content_point[0]:.1f}, {content_point[1]:.1f}], Different Transforms', fontsize=14)
    plt.tight_layout()
    plt.show()

if model_loaded:
    print("ğŸ”„ Exploring transform variations...")
    
    # Test different content points with varying transforms
    interesting_points = [[0, 0], [1, 1], [-1, 1], [2, -1]]
    
    for point in interesting_points:
        explore_with_transforms(model, device, content_point=point)
    
    print("âœ… Transform exploration complete!")
    
else:
    print("âŒ Cannot explore transforms without loaded model.")


# ğŸ“Š LATENT SPACE STATISTICS AND ANALYSIS

if model_loaded and 'content_data' in locals():
    print("ğŸ“Š LATENT SPACE ANALYSIS")
    print("="*50)
    
    # Analyze the latent space distributions
    content_mean = np.mean(content_data, axis=0)
    content_std = np.std(content_data, axis=0)
    transform_mean = np.mean(transform_data, axis=0)
    transform_std = np.std(transform_data, axis=0)
    
    print("ğŸ¯ CONTENT LATENT STATISTICS (2D):")
    print(f"   Dimension 1: Î¼={content_mean[0]:.3f}, Ïƒ={content_std[0]:.3f}")
    print(f"   Dimension 2: Î¼={content_mean[1]:.3f}, Ïƒ={content_std[1]:.3f}")
    
    print("\nğŸ”„ TRANSFORM LATENT STATISTICS (6D):")
    for i in range(6):
        print(f"   Dimension {i+1}: Î¼={transform_mean[i]:.3f}, Ïƒ={transform_std[i]:.3f}")
    
    # Correlation analysis
    content_corr = np.corrcoef(content_data.T)[0, 1]
    print(f"\nğŸ”— Content latent correlation: {content_corr:.3f}")
    
    # Digit class analysis
    if 'label_data' in locals():
        print(f"\nğŸ”¢ DIGIT CLASS DISTRIBUTION:")
        unique_labels, counts = np.unique(label_data, return_counts=True)
        for label, count in zip(unique_labels, counts):
            print(f"   Digit {label}: {count} samples ({count/len(label_data)*100:.1f}%)")
    
    # Create distribution plots
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    
    # Content latent distributions
    axes[0, 0].hist(content_data[:, 0], bins=50, alpha=0.7, color='blue')
    axes[0, 0].set_title('Content Latent Dim 1')
    axes[0, 0].axvline(content_mean[0], color='red', linestyle='--', label=f'Î¼={content_mean[0]:.2f}')
    axes[0, 0].legend()
    
    axes[0, 1].hist(content_data[:, 1], bins=50, alpha=0.7, color='green')
    axes[0, 1].set_title('Content Latent Dim 2')
    axes[0, 1].axvline(content_mean[1], color='red', linestyle='--', label=f'Î¼={content_mean[1]:.2f}')
    axes[0, 1].legend()
    
    # 2D scatter of content latent
    scatter = axes[0, 2].scatter(content_data[:, 0], content_data[:, 1], 
                                c=label_data if 'label_data' in locals() else 'blue', 
                                cmap='tab10', alpha=0.6, s=10)
    axes[0, 2].set_title('Content Latent Space')
    axes[0, 2].set_xlabel('Dimension 1')
    axes[0, 2].set_ylabel('Dimension 2')
    if 'label_data' in locals():
        plt.colorbar(scatter, ax=axes[0, 2])
    
    # Transform latent distributions (first 3 dimensions)
    for i in range(3):
        axes[1, i].hist(transform_data[:, i], bins=50, alpha=0.7, color='orange')
        axes[1, i].set_title(f'Transform Latent Dim {i+1}')
        axes[1, i].axvline(transform_mean[i], color='red', linestyle='--', 
                          label=f'Î¼={transform_mean[i]:.2f}')
        axes[1, i].legend()
    
    plt.tight_layout()
    plt.suptitle('ğŸ“Š Latent Space Distributions', fontsize=16, y=1.02)
    plt.show()
    
    print("âœ… Statistical analysis complete!")
    
else:
    print("âš ï¸ Statistical analysis requires loaded model and latent space data.")
    print("ğŸ’¡ Run the comprehensive visualization cell first to generate latent space data.")


# ğŸ’¾ SAVE VISUALIZATION RESULTS (Optional)

if model_loaded and VIZ_CONFIG.get('save_plots', False):
    print("ğŸ’¾ Saving visualization results...")
    
    import os
    from datetime import datetime
    
    # Create results directory
    results_dir = f"viz_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(results_dir, exist_ok=True)
    
    # Save model info
    model_info = {
        'model_path': model_path,
        'timestamp': viz_data.get('timestamp', 'unknown'),
        'model_type': viz_data.get('model_type', 'unknown'),
        'config': original_config,
        'final_loss': losses_dict.get('total_losses', [None])[-1] if losses_dict else None,
        'latent_space_range': VIZ_CONFIG['latent_range'],
        'grid_size': VIZ_CONFIG['grid_size']
    }
    
    import json
    with open(f"{results_dir}/model_info.json", 'w') as f:
        json.dump(model_info, f, indent=2, default=str)
    
    print(f"ğŸ“ Results saved to: {results_dir}/")
    print("   - model_info.json: Model and visualization metadata")
    print("   - (Plots would be saved here if matplotlib.pyplot.savefig was used)")
    
else:
    print("ğŸ’¡ To save results, set VIZ_CONFIG['save_plots'] = True in the configuration cell")

print("\nğŸ‰ VISUALIZATION NOTEBOOK COMPLETE!")
print("="*50)
print("âœ… Model loaded and analyzed")
print("ğŸ—ºï¸ Latent space grid scan created")
print("ğŸ¨ Comprehensive visualizations applied")
print("ğŸ”¬ Interactive exploration provided")
print("ğŸ“Š Statistical analysis performed")



