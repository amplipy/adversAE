"""
Affine-Invariant Autoencoder for MNIST - Shared Components
This module contains shared classes and functions used by both standard 64D and structured 2D+6D autoencoders.

Includes:
- AffineTransformationNetwork (shared by both model types)
- Loss functions (original and simplified)
- Data loading utilities
- Device management
- Model loading/saving utilities
- Common visualization functions
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import os
import json
import glob
from matplotlib.patches import Patch
from tqdm import tqdm
from datetime import datetime


class AffineTransformationNetwork(nn.Module):
    """
    Parallel network branch that predicts affine transformation parameters
    from the input image to make the autoencoder invariant to geometric transformations.
    """
    def __init__(self):
        super(AffineTransformationNetwork, self).__init__()
        
        # Convolutional feature extractor for affine parameters
        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)  # 28x28 -> 28x28
        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=2)  # 14x14 -> 14x14
        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 7x7 -> 7x7
        self.pool3 = nn.MaxPool2d(2, 2)  # 7x7 -> 3x3 (with some rounding)
        
        # Fully connected layers to predict affine parameters
        self.fc1 = nn.Linear(64 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, 64)
        
        # Output 6 affine transformation parameters [a, b, c, d, tx, ty]
        # for transformation matrix: [[a, b, tx], [c, d, ty], [0, 0, 1]]
        self.fc_affine = nn.Linear(64, 6)
        
        # Initialize affine parameters to identity transformation
        self.fc_affine.weight.data.zero_()
        self.fc_affine.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))
        
    def forward(self, x):
        # Extract features for affine parameter prediction
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        x = F.relu(self.conv3(x))
        x = self.pool3(x)
        
        # Flatten and predict affine parameters
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        
        # Output affine parameters
        affine_params = self.fc_affine(x)
        
        return affine_params


# =============================================================================
# SHARED LOSS FUNCTIONS
# =============================================================================

def affine_invariant_loss(original, reconstruction, transformed_reconstruction, affine_params, 
                         alpha=1.0, beta=0.1, gamma=0.01):
    """
    Combined loss function for affine-invariant autoencoder.
    
    Args:
        original: Original input images
        reconstruction: Direct autoencoder reconstruction
        transformed_reconstruction: Affine-transformed reconstruction
        affine_params: Predicted affine transformation parameters
        alpha: Weight for reconstruction loss
        beta: Weight for affine-invariant loss
        gamma: Weight for regularization loss
    
    Returns:
        Total loss, reconstruction loss, affine loss, regularization loss
    """
    # Standard reconstruction loss
    recon_loss = F.mse_loss(reconstruction, original)
    
    # Affine-invariant loss (transformed reconstruction should match original)
    affine_loss = F.mse_loss(transformed_reconstruction, original)
    
    # Regularization loss to prevent extreme transformations
    # Encourage affine parameters to stay close to identity transformation
    identity_params = torch.tensor([1, 0, 0, 0, 1, 0], device=affine_params.device, dtype=torch.float32).unsqueeze(0)
    identity_params = identity_params.expand_as(affine_params)
    reg_loss = F.mse_loss(affine_params, identity_params)
    
    # Combined loss
    total_loss = alpha * recon_loss + beta * affine_loss + gamma * reg_loss
    
    return total_loss, recon_loss, affine_loss, reg_loss


def simplified_affine_kl_loss(original, transformed_reconstruction, content_latent_mu, content_latent_logvar, 
                             alpha=1.0, beta=0.001):
    """
    Simplified loss function: affine-transformed reconstruction + KL divergence on 2D content latent.
    
    This is the clean, principled loss you wanted:
    - Affine transformation is applied to reconstruction before MSE with original
    - KL divergence encourages proper probabilistic structure in the 2D content latent space
    - No regularization on affine parameters or transform latent - let the model learn freely
    
    Args:
        original: Original input images [batch_size, 1, 28, 28]
        transformed_reconstruction: Affine-corrected reconstruction [batch_size, 1, 28, 28]
        content_latent_mu: Mean of content latent distribution [batch_size, 2]
        content_latent_logvar: Log variance of content latent distribution [batch_size, 2]
        alpha: Weight for reconstruction loss (typically 1.0)
        beta: Weight for KL divergence (typically small, e.g. 0.001-0.01)
    
    Returns:
        Total loss, reconstruction loss, KL divergence loss
    """
    # Reconstruction loss: MSE between affine-corrected reconstruction and original
    recon_loss = F.mse_loss(transformed_reconstruction, original)
    
    # KL divergence loss on 2D content latent space
    # KL(q(z|x) || p(z)) where p(z) = N(0,I) and q(z|x) = N(mu, sigma^2)
    kl_loss = -0.5 * torch.sum(1 + content_latent_logvar - content_latent_mu.pow(2) - content_latent_logvar.exp())
    kl_loss = kl_loss / original.size(0)  # Average over batch
    
    # Combined loss
    total_loss = alpha * recon_loss + beta * kl_loss
    
    return total_loss, recon_loss, kl_loss


# =============================================================================
# SHARED DATA LOADING FUNCTIONS
# =============================================================================

def get_mnist_loaders(batch_size_train=128, batch_size_test=64, data_dir='./data'):
    """Load MNIST dataset and return train and test data loaders"""
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])
    
    train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(data_dir, train=False, transform=transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)
    
    return train_loader, test_loader


def get_cloud_mnist_loaders(batch_size_train=256, batch_size_test=128, data_dir='../data', pin_memory=True, num_workers=4):
    """Get MNIST data loaders optimized for cloud training"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # Download to specified directory
    os.makedirs(data_dir, exist_ok=True)
    
    train_dataset = datasets.MNIST(data_dir, train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(data_dir, train=False, transform=transform)
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True, 
                             pin_memory=pin_memory, num_workers=num_workers)
    test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False,
                           pin_memory=pin_memory, num_workers=num_workers)
    
    print(f"ðŸ“Š Train batches: {len(train_loader)}, Test batches: {len(test_loader)}")
    return train_loader, test_loader


# =============================================================================
# SHARED DEVICE MANAGEMENT
# =============================================================================

def get_device(force_cpu_for_affine=False):
    """Get the appropriate device (GPU/MPS if available, else CPU)"""
    if force_cpu_for_affine:
        device = torch.device('cpu')
        print(f"Using device: cpu (forced for affine grid operations)")
    elif torch.mps.is_available():
        device = torch.device('mps')
        print(f"Using device: mps")
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"Using device: cuda")
    else:
        device = torch.device('cpu')
        print(f"Using device: cpu")
    return device


def get_cloud_device(config):
    """Get optimal device for cloud training with configuration options"""
    if config.get('force_cuda', False) and torch.cuda.is_available():
        device = torch.device('cuda')
        print(f"ðŸš€ Cloud CUDA device: {torch.cuda.get_device_name()}")
        print(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    elif torch.backends.mps.is_available():
        device = torch.device('mps')
        print("ðŸŽ Apple MPS device")
    else:
        device = torch.device('cpu')
        print("ðŸ’» CPU device")
    
    return device


# =============================================================================
# SHARED MODEL LOADING/SAVING FUNCTIONS
# =============================================================================

def save_model(model, filepath):
    """Save model state dict"""
    torch.save(model.state_dict(), filepath)
    print(f"Model saved to {filepath}")


def load_model(model, filepath, device):
    """Load model state dict"""
    model.load_state_dict(torch.load(filepath, map_location=device))
    model.to(device)
    print(f"Model loaded from {filepath}")
    return model


def save_model_cloud(model, config, losses, device):
    """Save standard model for cloud deployment"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_file = f"autoencoder_model_{timestamp}.pth"
    metadata_file = f"autoencoder_metadata_{timestamp}.json"
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config,
        'device': str(device)
    }, model_file)
    
    metadata = {
        'timestamp': timestamp,
        'config': config,
        'final_losses': {k: v[-1] if v else 0 for k, v in losses.items()}
    }
    
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"ðŸ’¾ Saved: {model_file}, {metadata_file}")
    return model_file, metadata_file


def list_saved_models(pattern="*model*.pth"):
    """List all saved model files in current directory"""
    
    model_files = glob.glob(pattern)
    model_files.sort(reverse=True)  # Most recent first
    
    if not model_files:
        print("ðŸ“ No saved models found")
        return []
    
    print(f"ðŸ“ Found {len(model_files)} saved models:")
    for i, file in enumerate(model_files):
        # Try to get timestamp from filename
        try:
            timestamp_part = file.split('_')[-1].replace('.pth', '')
            if len(timestamp_part) == 15:  # YYYYMMDD_HHMMSS format
                formatted_time = f"{timestamp_part[:8]}_{timestamp_part[9:]}"
                print(f"  {i+1}. {file} ({formatted_time})")
            else:
                print(f"  {i+1}. {file}")
        except:
            print(f"  {i+1}. {file}")
    
    return model_files


# =============================================================================
# SHARED TRAINING PROGRESS VISUALIZATION
# =============================================================================

def plot_training_progress(losses_dict):
    """Plot training loss progression"""
    fig, axes = plt.subplots(2, 2, figsize=(12, 8))
    
    # Total loss
    axes[0, 0].plot(losses_dict['total_losses'], 'b-', linewidth=2)
    axes[0, 0].set_title('Total Training Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Reconstruction loss
    axes[0, 1].plot(losses_dict['recon_losses'], 'r-', linewidth=2)
    axes[0, 1].set_title('Reconstruction Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].grid(True, alpha=0.3)
    
    # Affine loss
    axes[1, 0].plot(losses_dict['affine_losses'], 'g-', linewidth=2)
    axes[1, 0].set_title('Affine-Invariant Loss')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Regularization loss
    axes[1, 1].plot(losses_dict['reg_losses'], 'm-', linewidth=2)
    axes[1, 1].set_title('Regularization Loss')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()


def plot_simplified_training_progress(losses):
    """Plot training progress for simplified affine+KL loss"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Total loss
    axes[0].plot(losses['train_loss'], label='Train', color='blue', linewidth=2)
    axes[0].plot(losses['test_loss'], label='Test', color='red', linewidth=2)
    axes[0].set_title('Total Loss\n(Affine Reconstruction + KL)')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Reconstruction loss (MSE after affine)
    axes[1].plot(losses['recon_loss'], color='green', linewidth=2)
    axes[1].set_title('Reconstruction Loss\n(MSE after Affine Transform)')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].grid(True, alpha=0.3)
    
    # KL divergence loss
    axes[2].plot(losses['kl_loss'], color='purple', linewidth=2)
    axes[2].set_title('KL Divergence Loss\n(2D Content Latent)')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].grid(True, alpha=0.3)
    
    plt.suptitle('ðŸŽ¯ Simplified Loss Function Training Progress', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # Print final loss values
    print(f"\nðŸ“Š Final Loss Values:")
    print(f"  Total Train Loss: {losses['train_loss'][-1]:.6f}")
    print(f"  Total Test Loss: {losses['test_loss'][-1]:.6f}")
    print(f"  Reconstruction Loss: {losses['recon_loss'][-1]:.6f}")
    print(f"  KL Divergence Loss: {losses['kl_loss'][-1]:.6f}")


# =============================================================================
# SHARED GENERAL VISUALIZATION FUNCTIONS  
# =============================================================================

def show_random_reconstructions(model, test_loader, device, num_samples=8):
    """Show random digit reconstructions with original and reconstruction"""
    model.eval()
    
    # Get random test samples
    test_iter = iter(test_loader)
    test_images, test_labels = next(test_iter)
    
    # Select random samples
    indices = torch.randperm(len(test_images))[:num_samples]
    samples = test_images[indices].to(device)
    labels = test_labels[indices]
    
    with torch.no_grad():
        # This is a generic function - specific models will override the forward call
        model_output = model(samples)
        
        # Handle different model output formats
        if len(model_output) >= 4:
            # Models with affine outputs
            reconstructed = model_output[0]
            if len(model_output) > 4:
                # Structured model might have affine-corrected version
                try:
                    corrected = model_output[4]  # transformed_reconstruction
                except:
                    corrected = reconstructed
            else:
                # Standard model
                corrected = reconstructed
        else:
            reconstructed = model_output[0]
            corrected = reconstructed
    
    # Visualize results
    fig, axes = plt.subplots(3, num_samples, figsize=(num_samples * 2, 6))
    
    for i in range(num_samples):
        # Original
        axes[0, i].imshow(samples[i].cpu().squeeze(), cmap='gray')
        axes[0, i].set_title(f'Original\nDigit: {labels[i].item()}')
        axes[0, i].axis('off')
        
        # Reconstruction
        axes[1, i].imshow(reconstructed[i].cpu().squeeze(), cmap='gray')
        axes[1, i].set_title('Reconstruction')
        axes[1, i].axis('off')
        
        # Corrected (if available)
        axes[2, i].imshow(corrected[i].cpu().squeeze(), cmap='gray')
        axes[2, i].set_title('Final Output')
        axes[2, i].axis('off')
    
    plt.suptitle('ðŸŽ² Random Digit Reconstructions', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    print(f"ðŸŽ¯ Reconstructed {num_samples} random samples")


if __name__ == "__main__":
    print("This is the shared components module for affine-invariant autoencoders.")
    print("Import specific model classes from:")
    print("  - standard_64d_autoencoder.py for 64D models")
    print("  - structured_2d6d_autoencoder.py for 2D+6D models")
