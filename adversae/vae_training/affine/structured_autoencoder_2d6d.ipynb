{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5db0d7",
   "metadata": {},
   "source": [
    "# Structured Autoencoder: 2D Content + 6D Transform\n",
    "\n",
    "**Minimal Training Notebook - Updated for Modular Structure**\n",
    "\n",
    "- **2D Content Latent**: Digit identity/shape clustering\n",
    "- **6D Transform Latent**: Spatial transformations  \n",
    "- **Cloud Ready**: CUDA/CPU optimization\n",
    "- **Simplified Loss**: Uses new affine+KL loss function\n",
    "- **Explicit Imports**: Clear module references (structured_2d6d_autoencoder, affine_autoencoder_shared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ffb3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Explicit imports from modular structure\n",
    "import structured_2d6d_autoencoder as s2d6d\n",
    "import affine_autoencoder_shared as shared\n",
    "\n",
    "# üöÄ CONFIG - Updated for simplified loss\n",
    "CONFIG = {\n",
    "    'content_latent_dim': 2, 'transform_latent_dim': 6, 'total_latent_dim': 8,\n",
    "    'epochs': 50, 'learning_rate': 1e-3, 'batch_size_train': 256, 'batch_size_test': 128,\n",
    "    'alpha': 1.0,  # Affine loss weight\n",
    "    'beta': 0.001,  # KL divergence weight (reduced for 2D content latent)\n",
    "    'force_cuda': True, 'mixed_precision': True, 'gradient_clip': 1.0,\n",
    "    'pin_memory': True, 'num_workers': 4, 'weight_decay': 1e-5,\n",
    "    'lr_scheduler': True, 'early_stopping': True, 'patience': 10,\n",
    "    'data_dir': '../data', 'save_dir': './', 'checkpoint_freq': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b841f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçé Apple MPS device\n",
      "üìä Train batches: 235, Test batches: 79\n"
     ]
    }
   ],
   "source": [
    "# üå©Ô∏è SETUP - Using shared utilities with explicit module reference\n",
    "device = shared.get_cloud_device(CONFIG)\n",
    "scaler = torch.cuda.amp.GradScaler() if CONFIG['mixed_precision'] and device.type == 'cuda' else None\n",
    "train_loader, test_loader = shared.get_cloud_mnist_loaders(**{k: v for k, v in CONFIG.items() if k in ['batch_size_train', 'batch_size_test', 'data_dir', 'pin_memory', 'num_workers']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b764fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è MODEL - Using structured_2d6d_autoencoder module explicitly\n",
    "model = s2d6d.StructuredAffineInvariantAutoEncoder(\n",
    "    content_dim=CONFIG['content_latent_dim'],\n",
    "    transform_dim=CONFIG['transform_latent_dim']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712fae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ OPTIONAL: LOAD EXISTING MODEL (comment out to train new)\n",
    "# shared.list_saved_models()  # List available models (from affine_autoencoder_shared)\n",
    "# model, CONFIG, device = shared.load_model_cloud(\"structured_model_YYYYMMDD_HHMMSS.pth\")\n",
    "# Note: Use shared.* functions from affine_autoencoder_shared module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33de9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/235 [00:05<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::grid_sampler_2d_backward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# üöÄ TRAIN - Using simplified affine+KL loss from structured_2d6d_autoencoder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m losses_dict \u001b[38;5;241m=\u001b[39m s2d6d\u001b[38;5;241m.\u001b[39mtrain_structured_autoencoder_simplified(model, train_loader, test_loader, device, CONFIG)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Code/adversAE/adversae/vae_training/affine/structured_2d6d_autoencoder.py:167\u001b[0m, in \u001b[0;36mtrain_structured_autoencoder_simplified\u001b[0;34m(model, train_loader, test_loader, device, config, scaler)\u001b[0m\n\u001b[1;32m    165\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_clip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    169\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgradient_clip\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::grid_sampler_2d_backward' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "# üöÄ TRAIN - Using simplified affine+KL loss from structured_2d6d_autoencoder\n",
    "losses_dict = s2d6d.train_structured_autoencoder_simplified(model, train_loader, test_loader, device, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b734f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà VISUALIZE - Using explicit module references\n",
    "s2d6d.plot_simplified_training_progress_structured(losses_dict)\n",
    "content_data, transform_data, label_data = s2d6d.visualize_structured_latent_space(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea609de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé® COMPREHENSIVE VISUALIZATIONS - Using structured_2d6d_autoencoder module\n",
    "s2d6d.comprehensive_visualization_structured(model, test_loader, device, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ SAVE - Using shared save functionality from affine_autoencoder_shared\n",
    "# Basic save (model, config, losses only - no extra visualization data)\n",
    "model_file, metadata_file = shared.save_model_cloud(model, CONFIG, losses_dict, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üíæ ALTERNATIVE: Save for Visualization (includes all data needed for viz)\n",
    "# This saves everything needed to recreate visualizations later\n",
    "viz_filename = shared.save_model_for_viz(\n",
    "    model, \n",
    "    model_type=\"structured\",\n",
    "    config=CONFIG,\n",
    "    losses=losses_dict,\n",
    "    extra_data={\n",
    "        'content_data': content_data,\n",
    "        'transform_data': transform_data,\n",
    "        'label_data': label_data\n",
    "    },\n",
    "    name=\"2d6d_simplified\"  # Custom name for easy identification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f7fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ LOAD MODEL FOR VISUALIZATION\n",
    "# Run this cell to load a previously saved model instead of training from scratch\n",
    "\n",
    "# Option 1: Load specific model by filename\n",
    "# loaded_model, viz_data = shared.load_model_for_viz(\"structured_2d6d_simplified_20250720_123456.pth\", \n",
    "#                                                    s2d6d.StructuredAffineInvariantAutoEncoder, device)\n",
    "\n",
    "# Option 2: Quick load most recent model\n",
    "# loaded_model, viz_data = shared.quick_load_viz(s2d6d.StructuredAffineInvariantAutoEncoder, \n",
    "#                                                model_type=\"structured\", name=\"2d6d_simplified\", device=device)\n",
    "\n",
    "# After loading, you can access:\n",
    "# - loaded_model: The trained model ready for inference\n",
    "# - viz_data['config']: Original training configuration  \n",
    "# - viz_data['losses']: Training loss history\n",
    "# - viz_data['extra_data']: Latent embeddings and other visualization data\n",
    "\n",
    "print(\"üí° Uncomment the lines above to load a saved model for visualization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
