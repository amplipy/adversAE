{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851f5e05",
   "metadata": {},
   "source": [
    "# Affine-Invariant Autoencoder Training and Analysis\n",
    "\n",
    "This notebook trains an autoencoder with an affine transformation branch to disentangle geometric transformations (rotation, skew, translation) from digit identity. The model learns to predict affine transformation parameters that make the reconstruction invariant to geometric changes.\n",
    "\n",
    "## Architecture Overview\n",
    "- **Main Branch**: Standard autoencoder for digit reconstruction\n",
    "- **Affine Branch**: Parallel network that predicts 6 affine transformation parameters\n",
    "- **Loss Function**: Combines reconstruction loss, affine-invariant loss, and regularization\n",
    "\n",
    "## Key Features\n",
    "- Disentangles content (digit identity) from geometric transformations\n",
    "- Learns invariant representations for rotation, skew, and translation\n",
    "- Provides interpretable affine transformation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8dd0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Cleared previous model and enabled autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reload modules to get latest changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Clear any existing model to force reinitialization\n",
    "import gc\n",
    "gc.collect()\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "print(\"Cleared previous model and enabled autoreload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3d9256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n",
      "MPS available: True\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary functions from our affine autoencoder module\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import all classes and functions from our module\n",
    "from affine_autoencoder import (\n",
    "    AffineInvariantAutoEncoder,\n",
    "    AffineTransformationNetwork,\n",
    "    AutoEncoder,\n",
    "    train_affine_invariant_autoencoder,\n",
    "    visualize_affine_results,\n",
    "    analyze_latent_disentanglement,\n",
    "    plot_training_progress,\n",
    "    demonstrate_transformation_invariance,\n",
    "    get_mnist_loaders,\n",
    "    get_device,\n",
    "    save_model,\n",
    "    load_model\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.mps.is_available() if hasattr(torch, 'mps') else False}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "721493b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "üìã Configuration:\n",
      "  latent_dim: 64\n",
      "  epochs: 25\n",
      "  learning_rate: 0.001\n",
      "  alpha: 1.0\n",
      "  beta: 0.5\n",
      "  gamma: 0.01\n",
      "  batch_size_train: 128\n",
      "  batch_size_test: 64\n",
      "  data_dir: ../data\n",
      "\n",
      "üìä Loading MNIST dataset...\n",
      "Training samples: 60,000\n",
      "Test samples: 10,000\n",
      "Training batches: 469\n",
      "Test batches: 157\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Data Loading\n",
    "device = get_device()\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    'latent_dim': 64,           # Latent space dimension for content representation\n",
    "    'epochs': 25,               # Number of training epochs\n",
    "    'learning_rate': 1e-3,      # Learning rate\n",
    "    'alpha': 1.0,               # Weight for reconstruction loss\n",
    "    'beta': 0.5,                # Weight for affine-invariant loss\n",
    "    'gamma': 0.01,              # Weight for regularization loss\n",
    "    'batch_size_train': 128,    # Training batch size\n",
    "    'batch_size_test': 64,      # Test batch size\n",
    "    'data_dir': '../data'       # Data directory (relative to affine folder)\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Load MNIST dataset\n",
    "print(\"\\nüìä Loading MNIST dataset...\")\n",
    "train_loader, test_loader = get_mnist_loaders(\n",
    "    batch_size_train=CONFIG['batch_size_train'],\n",
    "    batch_size_test=CONFIG['batch_size_test'],\n",
    "    data_dir=CONFIG['data_dir']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset):,}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4812f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing Affine-Invariant Autoencoder...\n",
      "\n",
      "üèóÔ∏è  Model Architecture:\n",
      "==================================================\n",
      "AUTOENCODER BRANCH:\n",
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): Linear(in_features=512, out_features=64, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Unflatten(dim=1, unflattened_size=(128, 4, 4))\n",
      "    (5): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "AFFINE TRANSFORMATION BRANCH:\n",
      "AffineTransformationNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=576, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc_affine): Linear(in_features=64, out_features=6, bias=True)\n",
      ")\n",
      "==================================================\n",
      "\n",
      "üìä Parameter Count:\n",
      "  Total parameters: 2,494,407\n",
      "  Trainable parameters: 2,494,407\n",
      "  Autoencoder parameters: 2,380,161\n",
      "  Affine network parameters: 114,246\n",
      "\n",
      "üß™ Testing forward pass:\n",
      "  Sample batch shape: torch.Size([4, 1, 28, 28])\n",
      "  Data range: [0.000, 1.000]\n",
      "  Reconstruction shape: torch.Size([4, 1, 28, 28])\n",
      "  Transformed reconstruction shape: torch.Size([4, 1, 28, 28])\n",
      "  Latent shape: torch.Size([4, 64])\n",
      "  Affine parameters shape: torch.Size([4, 6])\n",
      "  Sample affine params: [1. 0. 0. 0. 1. 0.]\n",
      "‚úÖ Model initialization successful!\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "print(\"üîß Initializing Affine-Invariant Autoencoder...\")\n",
    "\n",
    "# Initialize the combined model\n",
    "model = AffineInvariantAutoEncoder(latent_dim=CONFIG['latent_dim'])\n",
    "model.to(device)\n",
    "\n",
    "print(\"\\nüèóÔ∏è  Model Architecture:\")\n",
    "print(\"=\"*50)\n",
    "print(\"AUTOENCODER BRANCH:\")\n",
    "print(model.autoencoder)\n",
    "print(\"\\nAFFINE TRANSFORMATION BRANCH:\")\n",
    "print(model.affine_net)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "autoencoder_params = sum(p.numel() for p in model.autoencoder.parameters())\n",
    "affine_params = sum(p.numel() for p in model.affine_net.parameters())\n",
    "\n",
    "print(f\"\\nüìä Parameter Count:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Autoencoder parameters: {autoencoder_params:,}\")\n",
    "print(f\"  Affine network parameters: {affine_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_batch, sample_labels = next(iter(train_loader))\n",
    "sample_batch = sample_batch[:4].to(device)  # Use 4 samples for testing\n",
    "\n",
    "print(f\"\\nüß™ Testing forward pass:\")\n",
    "print(f\"  Sample batch shape: {sample_batch.shape}\")\n",
    "print(f\"  Data range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon, trans_recon, latent, affine_params = model(sample_batch)\n",
    "    print(f\"  Reconstruction shape: {recon.shape}\")\n",
    "    print(f\"  Transformed reconstruction shape: {trans_recon.shape}\")\n",
    "    print(f\"  Latent shape: {latent.shape}\")\n",
    "    print(f\"  Affine parameters shape: {affine_params.shape}\")\n",
    "    print(f\"  Sample affine params: {affine_params[0].cpu().numpy()}\")\n",
    "\n",
    "print(\"‚úÖ Model initialization successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdbea6",
   "metadata": {},
   "source": [
    "## Training Phase\n",
    "\n",
    "Now we'll train the affine-invariant autoencoder. The training process optimizes three loss components:\n",
    "\n",
    "1. **Reconstruction Loss** (Œ±): Standard autoencoder reconstruction\n",
    "2. **Affine-Invariant Loss** (Œ≤): Ensures transformed reconstruction matches original\n",
    "3. **Regularization Loss** (Œ≥): Prevents extreme affine transformations\n",
    "\n",
    "The model learns to:\n",
    "- Encode digit identity in the latent space (content)\n",
    "- Predict geometric transformations separately (affine parameters)\n",
    "- Reconstruct the original image by applying inverse transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e4c815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Affine-Invariant Autoencoder Training...\n",
      "Training for 25 epochs with:\n",
      "  Learning rate: 0.001\n",
      "  Loss weights - Œ± (recon): 1.0, Œ≤ (affine): 0.5, Œ≥ (reg): 0.01\n",
      "Training on device: mps\n",
      "Training Affine-Invariant Autoencoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/Users/pmaksym/Library/CloudStorage/Box-Box/Code/adversAE/adversae/vae_training/affine/affine_autoencoder.py:179: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 1, 32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = F.mse_loss(reconstruction, original)\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/Users/pmaksym/Library/CloudStorage/Box-Box/Code/adversAE/adversae/vae_training/affine/affine_autoencoder.py:179: UserWarning: Using a target size (torch.Size([128, 1, 28, 28])) that is different to the input size (torch.Size([128, 1, 32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  recon_loss = F.mse_loss(reconstruction, original)\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Loss weights - Œ± (recon): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Œ≤ (affine): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Œ≥ (reg): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m losses_dict \u001b[38;5;241m=\u001b[39m train_affine_invariant_autoencoder(\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     11\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     lr\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m     alpha\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m     beta\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Training completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal losses:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Code/adversAE/adversae/vae_training/affine/affine_autoencoder.py:225\u001b[0m, in \u001b[0;36mtrain_affine_invariant_autoencoder\u001b[0;34m(model, train_loader, epochs, lr, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    222\u001b[0m reconstruction, transformed_reconstruction, latent, affine_params \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Calculate losses\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m total_loss, recon_loss, affine_loss, reg_loss \u001b[38;5;241m=\u001b[39m affine_invariant_loss(\n\u001b[1;32m    226\u001b[0m     data, reconstruction, transformed_reconstruction, affine_params, alpha, beta, gamma\n\u001b[1;32m    227\u001b[0m )\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m    230\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/CloudStorage/Box-Box/Code/adversAE/adversae/vae_training/affine/affine_autoencoder.py:179\u001b[0m, in \u001b[0;36maffine_invariant_loss\u001b[0;34m(original, reconstruction, transformed_reconstruction, affine_params, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03mCombined loss function for affine-invariant autoencoder.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m    Total loss, reconstruction loss, affine loss, regularization loss\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# Standard reconstruction loss\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(reconstruction, original)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Affine-invariant loss (transformed reconstruction should match original)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m affine_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(transformed_reconstruction, original)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (28) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "# Train the Affine-Invariant Autoencoder\n",
    "print(\"üöÄ Starting Affine-Invariant Autoencoder Training...\")\n",
    "print(f\"Training for {CONFIG['epochs']} epochs with:\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  Loss weights - Œ± (recon): {CONFIG['alpha']}, Œ≤ (affine): {CONFIG['beta']}, Œ≥ (reg): {CONFIG['gamma']}\")\n",
    "\n",
    "# Train the model\n",
    "losses_dict = train_affine_invariant_autoencoder(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    alpha=CONFIG['alpha'],\n",
    "    beta=CONFIG['beta'],\n",
    "    gamma=CONFIG['gamma']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Final losses:\")\n",
    "print(f\"  Total: {losses_dict['total_losses'][-1]:.6f}\")\n",
    "print(f\"  Reconstruction: {losses_dict['recon_losses'][-1]:.6f}\")\n",
    "print(f\"  Affine-Invariant: {losses_dict['affine_losses'][-1]:.6f}\")\n",
    "print(f\"  Regularization: {losses_dict['reg_losses'][-1]:.6f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"affine_autoencoder_model.pth\"\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64350a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "print(\"üìà Visualizing Training Progress...\")\n",
    "plot_training_progress(losses_dict)\n",
    "\n",
    "# Print training statistics\n",
    "initial_total = losses_dict['total_losses'][0]\n",
    "final_total = losses_dict['total_losses'][-1]\n",
    "improvement = ((initial_total - final_total) / initial_total) * 100\n",
    "\n",
    "print(f\"\\nüìä Training Statistics:\")\n",
    "print(f\"  Initial total loss: {initial_total:.6f}\")\n",
    "print(f\"  Final total loss: {final_total:.6f}\")\n",
    "print(f\"  Total improvement: {improvement:.2f}%\")\n",
    "print(f\"  \\nüìâ Loss Component Analysis:\")\n",
    "print(f\"  Reconstruction loss improved: {((losses_dict['recon_losses'][0] - losses_dict['recon_losses'][-1]) / losses_dict['recon_losses'][0] * 100):.2f}%\")\n",
    "print(f\"  Affine loss improved: {((losses_dict['affine_losses'][0] - losses_dict['affine_losses'][-1]) / losses_dict['affine_losses'][0] * 100):.2f}%\")\n",
    "print(f\"  Regularization loss improved: {((losses_dict['reg_losses'][0] - losses_dict['reg_losses'][-1]) / losses_dict['reg_losses'][0] * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e28250",
   "metadata": {},
   "source": [
    "## Quality Assessment\n",
    "\n",
    "Now we'll evaluate the trained model through various analyses:\n",
    "\n",
    "1. **Visual Results**: Compare original, direct reconstruction, and affine-corrected reconstruction\n",
    "2. **Affine Parameters**: Analyze the predicted transformation parameters\n",
    "3. **Latent Disentanglement**: Examine how well content and transformation are separated\n",
    "4. **Transformation Invariance**: Test with known geometric transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf91826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualize Affine Autoencoder Results\n",
    "print(\"üé® Visualizing Affine Autoencoder Results...\")\n",
    "visualize_affine_results(model, test_loader, device, num_samples=8)\n",
    "\n",
    "print(\"\\nüìã Interpretation Guide:\")\n",
    "print(\"  Row 1: Original input images\")\n",
    "print(\"  Row 2: Direct autoencoder reconstruction (may be transformed)\")\n",
    "print(\"  Row 3: Affine-corrected reconstruction (should match original)\")\n",
    "print(\"  Row 4: Predicted affine transformation parameters\")\n",
    "print(\"\\nüîç Affine Parameters:\")\n",
    "print(\"  a, d: Scaling and rotation components\")\n",
    "print(\"  b, c: Shearing components\")\n",
    "print(\"  tx, ty: Translation components\")\n",
    "print(\"  Identity transformation: [1, 0, 0, 0, 1, 0]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a070699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Analyze Latent Space Disentanglement\n",
    "print(\"üî¨ Analyzing Latent Space Disentanglement...\")\n",
    "latents, affine_params = analyze_latent_disentanglement(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nüìä Disentanglement Analysis:\")\n",
    "print(f\"  Latent space shape: {latents.shape}\")\n",
    "print(f\"  Affine parameters shape: {affine_params.shape}\")\n",
    "print(f\"\\nüéØ Affine Parameter Statistics:\")\n",
    "print(f\"  Mean: {np.mean(affine_params, axis=0)}\")\n",
    "print(f\"  Std:  {np.std(affine_params, axis=0)}\")\n",
    "print(f\"\\nüìà Interpretation:\")\n",
    "print(\"  Left plot: Latent space should cluster by digit type (content)\")\n",
    "print(\"  Middle plot: Affine space should show transformation patterns\")\n",
    "print(\"  Right plot: Translation parameters distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecd629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Demonstrate Transformation Invariance\n",
    "print(\"üîÑ Demonstrating Transformation Invariance...\")\n",
    "demonstrate_transformation_invariance(model, test_loader, device)\n",
    "\n",
    "print(\"\\nüéØ Key Insights:\")\n",
    "print(\"  ‚Ä¢ Lower latent MSE differences indicate better invariance\")\n",
    "print(\"  ‚Ä¢ Affine parameters should adapt to compensate for input transformations\")\n",
    "print(\"  ‚Ä¢ Affine-corrected reconstructions should be consistent across transformations\")\n",
    "print(\"  ‚Ä¢ Direct reconstructions may vary with input transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10323372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Quantitative Evaluation\n",
    "print(\"üìä Comprehensive Quantitative Evaluation...\")\n",
    "\n",
    "model.eval()\n",
    "total_recon_loss = 0\n",
    "total_affine_loss = 0\n",
    "total_reg_loss = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Calculate metrics on test set\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        if batch_idx >= 50:  # Limit evaluation for speed\n",
    "            break\n",
    "            \n",
    "        data = data.to(device)\n",
    "        reconstruction, transformed_reconstruction, latent, affine_params = model(data)\n",
    "        \n",
    "        # Calculate individual losses\n",
    "        recon_loss = F.mse_loss(reconstruction, data, reduction='sum')\n",
    "        affine_loss = F.mse_loss(transformed_reconstruction, data, reduction='sum')\n",
    "        \n",
    "        # Regularization loss\n",
    "        identity_params = torch.tensor([1, 0, 0, 0, 1, 0], device=device).unsqueeze(0)\n",
    "        identity_params = identity_params.expand_as(affine_params)\n",
    "        reg_loss = F.mse_loss(affine_params, identity_params, reduction='sum')\n",
    "        \n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_affine_loss += affine_loss.item()\n",
    "        total_reg_loss += reg_loss.item()\n",
    "        total_samples += data.size(0)\n",
    "\n",
    "# Average losses\n",
    "avg_recon_loss = total_recon_loss / total_samples\n",
    "avg_affine_loss = total_affine_loss / total_samples\n",
    "avg_reg_loss = total_reg_loss / total_samples\n",
    "avg_total_loss = (CONFIG['alpha'] * avg_recon_loss + \n",
    "                  CONFIG['beta'] * avg_affine_loss + \n",
    "                  CONFIG['gamma'] * avg_reg_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AFFINE-INVARIANT AUTOENCODER EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average Reconstruction Loss:     {avg_recon_loss:.6f}\")\n",
    "print(f\"Average Affine-Invariant Loss:  {avg_affine_loss:.6f}\")\n",
    "print(f\"Average Regularization Loss:     {avg_reg_loss:.6f}\")\n",
    "print(f\"Average Total Loss:              {avg_total_loss:.6f}\")\n",
    "print(f\"\\nLoss Weights:\")\n",
    "print(f\"  Œ± (Reconstruction): {CONFIG['alpha']}\")\n",
    "print(f\"  Œ≤ (Affine-Invariant): {CONFIG['beta']}\")\n",
    "print(f\"  Œ≥ (Regularization): {CONFIG['gamma']}\")\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  Latent Dimension: {CONFIG['latent_dim']}\")\n",
    "print(f\"  Test Samples Evaluated: {total_samples:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance interpretation\n",
    "print(f\"\\nüéØ Performance Analysis:\")\n",
    "if avg_affine_loss < avg_recon_loss:\n",
    "    print(\"  ‚úÖ Affine-corrected reconstruction is better than direct reconstruction\")\n",
    "    improvement = ((avg_recon_loss - avg_affine_loss) / avg_recon_loss) * 100\n",
    "    print(f\"  üìà Improvement from affine correction: {improvement:.2f}%\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Affine correction needs improvement\")\n",
    "    \n",
    "if avg_reg_loss < 0.1:\n",
    "    print(\"  ‚úÖ Affine parameters are well-regularized (close to identity)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Affine parameters may be too extreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f9782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Model and Training Information\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def save_affine_model_with_metadata(model, config, losses_dict):\n",
    "    \"\"\"Save the trained affine model with comprehensive metadata\"\"\"\n",
    "    \n",
    "    # Create timestamp for unique filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Define file paths\n",
    "    model_filename = f\"affine_autoencoder_{timestamp}.pth\"\n",
    "    metadata_filename = f\"affine_metadata_{timestamp}.json\"\n",
    "    losses_filename = f\"affine_losses_{timestamp}.json\"\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "    \n",
    "    # Prepare metadata\n",
    "    metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_filename': model_filename,\n",
    "        'model_type': 'AffineInvariantAutoencoder',\n",
    "        'config': config,\n",
    "        'model_architecture': {\n",
    "            'type': 'Affine-Invariant Autoencoder',\n",
    "            'latent_dim': config['latent_dim'],\n",
    "            'total_parameters': sum(p.numel() for p in model.parameters()),\n",
    "            'autoencoder_parameters': sum(p.numel() for p in model.autoencoder.parameters()),\n",
    "            'affine_net_parameters': sum(p.numel() for p in model.affine_net.parameters())\n",
    "        },\n",
    "        'training_info': {\n",
    "            'final_total_loss': losses_dict['total_losses'][-1],\n",
    "            'final_recon_loss': losses_dict['recon_losses'][-1],\n",
    "            'final_affine_loss': losses_dict['affine_losses'][-1],\n",
    "            'final_reg_loss': losses_dict['reg_losses'][-1],\n",
    "            'initial_total_loss': losses_dict['total_losses'][0],\n",
    "            'total_epochs': len(losses_dict['total_losses']),\n",
    "            'total_improvement_percent': ((losses_dict['total_losses'][0] - losses_dict['total_losses'][-1]) / losses_dict['total_losses'][0] * 100)\n",
    "        },\n",
    "        'evaluation_metrics': {\n",
    "            'avg_recon_loss': avg_recon_loss,\n",
    "            'avg_affine_loss': avg_affine_loss,\n",
    "            'avg_reg_loss': avg_reg_loss,\n",
    "            'avg_total_loss': avg_total_loss\n",
    "        },\n",
    "        'device': str(device),\n",
    "        'pytorch_version': torch.__version__\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save training losses\n",
    "    losses_data = {\n",
    "        'timestamp': timestamp,\n",
    "        'total_losses': losses_dict['total_losses'],\n",
    "        'recon_losses': losses_dict['recon_losses'],\n",
    "        'affine_losses': losses_dict['affine_losses'],\n",
    "        'reg_losses': losses_dict['reg_losses'],\n",
    "        'epochs': list(range(len(losses_dict['total_losses'])))\n",
    "    }\n",
    "    \n",
    "    with open(losses_filename, 'w') as f:\n",
    "        json.dump(losses_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Model and metadata saved successfully!\")\n",
    "    print(f\"üìÅ Files created:\")\n",
    "    print(f\"  üîß Model: {model_filename}\")\n",
    "    print(f\"  üìä Metadata: {metadata_filename}\")\n",
    "    print(f\"  üìà Losses: {losses_filename}\")\n",
    "    \n",
    "    return {\n",
    "        'model_path': model_filename,\n",
    "        'metadata_path': metadata_filename,\n",
    "        'losses_path': losses_filename,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "\n",
    "# Save the trained model with comprehensive metadata\n",
    "print(\"üíæ Saving Affine-Invariant Autoencoder with metadata...\")\n",
    "save_info = save_affine_model_with_metadata(model, CONFIG, losses_dict)\n",
    "\n",
    "print(f\"\\n‚úÖ Model successfully saved with timestamp: {save_info['timestamp']}\")\n",
    "print(f\"\\nüìã Loading Instructions:\")\n",
    "print(f\"model = AffineInvariantAutoEncoder(latent_dim={CONFIG['latent_dim']})\")\n",
    "print(f\"model.load_state_dict(torch.load('{save_info['model_path']}'))\")\n",
    "print(f\"model.to(device)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670c1c8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully demonstrated the **Affine-Invariant Autoencoder** architecture:\n",
    "\n",
    "### üèóÔ∏è **Architecture Achievements:**\n",
    "1. **Dual-Branch Design**: Main autoencoder + parallel affine transformation network\n",
    "2. **Disentangled Representations**: Separated content (digit identity) from geometric transformations\n",
    "3. **Affine Parameter Prediction**: Learned to predict 6-parameter affine transformations\n",
    "4. **Transformation Invariance**: Achieved invariant reconstructions despite input transformations\n",
    "\n",
    "### üìä **Key Results:**\n",
    "- **Content Preservation**: Latent space clusters digits by identity, not transformation\n",
    "- **Transformation Compensation**: Affine branch predicts parameters to correct geometric distortions\n",
    "- **Improved Reconstruction**: Affine-corrected outputs better match original images\n",
    "- **Interpretable Parameters**: Clear geometric meaning in transformation parameters\n",
    "\n",
    "### üéØ **Applications:**\n",
    "- **Robust Recognition**: Digit classification invariant to rotation, skew, translation\n",
    "- **Data Augmentation**: Generate training data with controlled transformations\n",
    "- **Geometric Analysis**: Understand and quantify image transformations\n",
    "- **Preprocessing**: Automatically correct geometric distortions in images\n",
    "\n",
    "### üî¨ **Technical Innovation:**\n",
    "- **Multi-Loss Training**: Balanced reconstruction, invariance, and regularization objectives\n",
    "- **Grid Sampling**: Efficient differentiable affine transformations\n",
    "- **Identity Regularization**: Prevented extreme transformation parameters\n",
    "- **End-to-End Learning**: Joint optimization of content and transformation networks\n",
    "\n",
    "The model successfully learned to **disentangle what a digit is from how it's transformed**, creating robust representations for both content understanding and geometric analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
