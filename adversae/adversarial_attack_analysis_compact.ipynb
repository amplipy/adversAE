{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64865a8",
   "metadata": {},
   "source": [
    "# Adversarial Attack Analysis on VAE\n",
    "\n",
    "This notebook loads a pre-trained VAE model and performs comprehensive adversarial attack analysis using functions from the `attack_analysis` module.\n",
    "\n",
    "## Analysis Components:\n",
    "1. **Visual Attack Analysis**: 6-row visualization showing original, adversarial, and difference images\n",
    "2. **Attack Method Comparison**: Effectiveness comparison across FGSM, PGD, and Custom attacks\n",
    "3. **Efficiency Scaling**: How attack success scales with epsilon parameter\n",
    "4. **Latent Space Analysis**: How attacks affect digit clustering in latent space\n",
    "\n",
    "## Attack Methods:\n",
    "- **FGSM** (Fast Gradient Sign Method)\n",
    "- **PGD** (Projected Gradient Descent)\n",
    "- **Custom Iterative Attack**\n",
    "- **Latent Space Attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa909118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Import VAE and attack classes\n",
    "from adversarial_vae_attack import (\n",
    "    VAE, \n",
    "    AdversarialAttacks,\n",
    "    get_mnist_loaders, \n",
    "    get_device,\n",
    "    vae_loss\n",
    ")\n",
    "\n",
    "# Import attack analysis functions\n",
    "from attack_analysis import (\n",
    "    visualize_attack_analysis,\n",
    "    custom_vae_attack,\n",
    "    compare_attack_effectiveness,\n",
    "    analyze_attack_success_rates,\n",
    "    comprehensive_attack_efficiency,\n",
    "    plot_attack_efficiency_scaling,\n",
    "    plot_simple_efficiency_summary,\n",
    "    sample_digits_by_class,\n",
    "    analyze_latent_clustering_under_attack,\n",
    "    plot_latent_clustering_comparison,\n",
    "    compare_latent_disruption_across_attacks,\n",
    "    save_analysis_results\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301785a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration\n",
    "device = get_device()\n",
    "\n",
    "CONFIG = {\n",
    "    'latent_dim': 2,\n",
    "    'batch_size_test': 64,\n",
    "    'data_dir': './data',\n",
    "    'n_attack_samples': 8,  # Number of samples for visualization\n",
    "    'attack_epsilons': [0.05, 0.1, 0.15, 0.2],  # Perturbation strengths\n",
    "    'efficiency_samples': 1000,  # Samples for efficiency analysis\n",
    "    'clustering_samples': 12    # Samples per digit for clustering analysis\n",
    "}\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9782c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the most recent trained model\n",
    "def load_latest_model():\n",
    "    \"\"\"Load the most recently saved VAE model and its metadata\"\"\"\n",
    "    model_files = glob.glob(\"vae_model_*.pth\")\n",
    "    \n",
    "    if not model_files:\n",
    "        raise FileNotFoundError(\"No trained VAE models found. Please run vae_trainer1.ipynb first.\")\n",
    "    \n",
    "    model_files.sort()\n",
    "    latest_model_file = model_files[-1]\n",
    "    timestamp = latest_model_file.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_file = f\"vae_metadata_{timestamp}.json\"\n",
    "    metadata = None\n",
    "    if os.path.exists(metadata_file):\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "    \n",
    "    # Initialize and load model\n",
    "    model = VAE(latent_dim=CONFIG['latent_dim'])\n",
    "    model.load_state_dict(torch.load(latest_model_file, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"üìÅ Loaded model: {latest_model_file}\")\n",
    "    if metadata:\n",
    "        print(f\"üïí Training timestamp: {metadata['timestamp']}\")\n",
    "        print(f\"üìä Final training loss: {metadata['training_info']['final_loss']:.6f}\")\n",
    "        print(f\"üéØ Loss reduction: {metadata['training_info']['loss_reduction_percent']:.2f}%\")\n",
    "    \n",
    "    return model, metadata, timestamp\n",
    "\n",
    "# Load model and data\n",
    "model, metadata, model_timestamp = load_latest_model()\n",
    "\n",
    "print(\"\\nüìä Loading MNIST test dataset...\")\n",
    "_, test_loader = get_mnist_loaders(\n",
    "    batch_size_train=128,\n",
    "    batch_size_test=CONFIG['batch_size_test'],\n",
    "    data_dir=CONFIG['data_dir']\n",
    ")\n",
    "\n",
    "# Get sample data\n",
    "test_iter = iter(test_loader)\n",
    "test_images, test_labels = next(test_iter)\n",
    "test_images = test_images.to(device)\n",
    "test_labels = test_labels.to(device)\n",
    "\n",
    "sample_images = test_images[:CONFIG['n_attack_samples']]\n",
    "sample_labels = test_labels[:CONFIG['n_attack_samples']]\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {len(test_loader.dataset)} test samples\")\n",
    "print(f\"üìù Sample labels: {sample_labels.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898216c6",
   "metadata": {},
   "source": [
    "## üî• FGSM Attack Analysis\n",
    "\n",
    "Fast Gradient Sign Method - simple but effective single-step attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fe737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FGSM Attack Analysis\n",
    "print(\"üî• Performing FGSM Attack Analysis...\")\n",
    "\n",
    "attacks = AdversarialAttacks()\n",
    "\n",
    "# Get original reconstructions\n",
    "with torch.no_grad():\n",
    "    original_recons, _, _ = model(sample_images)\n",
    "\n",
    "# Test different epsilon values\n",
    "for epsilon in CONFIG['attack_epsilons']:\n",
    "    print(f\"\\nüéØ Testing FGSM with epsilon = {epsilon}\")\n",
    "    \n",
    "    # Perform FGSM attack\n",
    "    fgsm_images = attacks.fgsm_attack(model, sample_images, sample_images, epsilon)\n",
    "    \n",
    "    # Get adversarial reconstructions\n",
    "    with torch.no_grad():\n",
    "        fgsm_recons, _, _ = model(fgsm_images)\n",
    "    \n",
    "    # Visualize attack analysis\n",
    "    visualize_attack_analysis(\n",
    "        sample_images, original_recons, fgsm_images, fgsm_recons,\n",
    "        \"FGSM\", epsilon, sample_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca1b20",
   "metadata": {},
   "source": [
    "## üî• PGD Attack Analysis\n",
    "\n",
    "Projected Gradient Descent - iterative attack with multiple refinement steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD Attack Analysis\n",
    "print(\"üî• Performing PGD Attack Analysis...\")\n",
    "\n",
    "for epsilon in CONFIG['attack_epsilons']:\n",
    "    print(f\"\\nüéØ Testing PGD with epsilon = {epsilon}\")\n",
    "    \n",
    "    # Perform PGD attack\n",
    "    pgd_images = attacks.pgd_attack(\n",
    "        model, sample_images, sample_images, \n",
    "        epsilon=epsilon, alpha=epsilon/10, num_iter=20\n",
    "    )\n",
    "    \n",
    "    # Get adversarial reconstructions\n",
    "    with torch.no_grad():\n",
    "        pgd_recons, _, _ = model(pgd_images)\n",
    "    \n",
    "    # Visualize attack analysis\n",
    "    visualize_attack_analysis(\n",
    "        sample_images, original_recons, pgd_images, pgd_recons,\n",
    "        \"PGD\", epsilon, sample_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c494b",
   "metadata": {},
   "source": [
    "## üî• Latent Space Attack Analysis\n",
    "\n",
    "Attack that directly manipulates latent space representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddfab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space Attack Analysis\n",
    "print(\"üî• Performing Latent Space Attack Analysis...\")\n",
    "\n",
    "latent_epsilons = [0.5, 1.0, 2.0, 3.0]\n",
    "\n",
    "for epsilon in latent_epsilons:\n",
    "    print(f\"\\nüéØ Testing Latent Attack with epsilon = {epsilon}\")\n",
    "    \n",
    "    # Perform latent space attack\n",
    "    latent_images, orig_latent, perturbed_latent = attacks.latent_space_attack(\n",
    "        model, sample_images, epsilon\n",
    "    )\n",
    "    \n",
    "    # Visualize attack analysis\n",
    "    visualize_attack_analysis(\n",
    "        sample_images, original_recons, latent_images, latent_images,\n",
    "        \"Latent Space\", epsilon, sample_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97114ff",
   "metadata": {},
   "source": [
    "## üî• Custom Iterative Attack Analysis\n",
    "\n",
    "Custom attack combining reconstruction and KL divergence targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Iterative Attack Analysis\n",
    "print(\"üî• Performing Custom Iterative Attack Analysis...\")\n",
    "\n",
    "for epsilon in CONFIG['attack_epsilons']:\n",
    "    print(f\"\\nüéØ Testing Custom Attack with epsilon = {epsilon}\")\n",
    "    \n",
    "    # Perform custom attack\n",
    "    custom_images = custom_vae_attack(model, sample_images, epsilon, num_iter=15)\n",
    "    \n",
    "    # Get adversarial reconstructions\n",
    "    with torch.no_grad():\n",
    "        custom_recons, _, _ = model(custom_images)\n",
    "    \n",
    "    # Visualize attack analysis\n",
    "    visualize_attack_analysis(\n",
    "        sample_images, original_recons, custom_images, custom_recons,\n",
    "        \"Custom Iterative\", epsilon, sample_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766eb37",
   "metadata": {},
   "source": [
    "## üìä Attack Effectiveness Comparison\n",
    "\n",
    "Compare different attack methods across various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c2b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack Effectiveness Comparison\n",
    "print(\"üìä Comparing Attack Effectiveness...\")\n",
    "\n",
    "comparison_results = compare_attack_effectiveness(model, sample_images, epsilon=0.1)\n",
    "\n",
    "# Display results in table format\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ATTACK EFFECTIVENESS COMPARISON (Œµ=0.1)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Attack':<15} {'Img MSE':<10} {'Img L‚àû':<10} {'Recon MSE':<12} {'Latent MSE':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for attack_name, metrics in comparison_results.items():\n",
    "    print(f\"{attack_name:<15} {metrics['img_mse']:<10.6f} {metrics['img_linf']:<10.6f} \"\n",
    "          f\"{metrics['recon_mse']:<12.6f} {metrics['latent_mse']:<12.6f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Metrics:\")\n",
    "print(\"- Img MSE: Mean squared error between original and adversarial images\")\n",
    "print(\"- Img L‚àû: Maximum absolute perturbation (should be ‚â§ Œµ)\")\n",
    "print(\"- Recon MSE: Difference in VAE reconstructions\")\n",
    "print(\"- Latent MSE: Difference in latent space representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81063790",
   "metadata": {},
   "source": [
    "## üìà Attack Success Rate Analysis\n",
    "\n",
    "Analyze how attack success varies with perturbation strength and across different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81892da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack Success Rate Analysis\n",
    "print(\"üìà Analyzing Attack Success Rates...\")\n",
    "\n",
    "success_results, total_tested = analyze_attack_success_rates(\n",
    "    model, test_loader, CONFIG['attack_epsilons'], device, max_batches=5\n",
    ")\n",
    "\n",
    "print(f\"\\nTested on {total_tested} samples\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ATTACK SUCCESS RATES BY EPSILON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Epsilon':<10} {'FGSM Success':<15} {'PGD Success':<15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for eps in CONFIG['attack_epsilons']:\n",
    "    fgsm_rate = np.mean(success_results[eps]['fgsm']) * 100\n",
    "    pgd_rate = np.mean(success_results[eps]['pgd']) * 100\n",
    "    print(f\"{eps:<10} {fgsm_rate:<15.1f}% {pgd_rate:<15.1f}%\")\n",
    "\n",
    "# Plot success rates\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Overall success rates\n",
    "plt.subplot(1, 2, 1)\n",
    "epsilons = CONFIG['attack_epsilons']\n",
    "fgsm_rates = [np.mean(success_results[eps]['fgsm']) * 100 for eps in epsilons]\n",
    "pgd_rates = [np.mean(success_results[eps]['pgd']) * 100 for eps in epsilons]\n",
    "\n",
    "plt.plot(epsilons, fgsm_rates, 'o-', label='FGSM', linewidth=2, markersize=8)\n",
    "plt.plot(epsilons, pgd_rates, 's-', label='PGD', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (Perturbation Strength)')\n",
    "plt.ylabel('Attack Success Rate (%)')\n",
    "plt.title('Attack Success Rate vs Epsilon')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Success rates by digit (for epsilon=0.1)\n",
    "plt.subplot(1, 2, 2)\n",
    "eps = 0.1\n",
    "digits = list(range(10))\n",
    "fgsm_by_digit = [np.mean(success_results[eps]['by_digit'][d]['fgsm']) * 100 \n",
    "                 if success_results[eps]['by_digit'][d]['fgsm'] else 0 for d in digits]\n",
    "pgd_by_digit = [np.mean(success_results[eps]['by_digit'][d]['pgd']) * 100 \n",
    "                if success_results[eps]['by_digit'][d]['pgd'] else 0 for d in digits]\n",
    "\n",
    "x = np.arange(len(digits))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, fgsm_by_digit, width, label='FGSM', alpha=0.8)\n",
    "plt.bar(x + width/2, pgd_by_digit, width, label='PGD', alpha=0.8)\n",
    "plt.xlabel('Digit Class')\n",
    "plt.ylabel('Attack Success Rate (%)')\n",
    "plt.title(f'Attack Success by Digit Class (Œµ={eps})')\n",
    "plt.xticks(x, digits)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68dc56c",
   "metadata": {},
   "source": [
    "## üöÄ Comprehensive Attack Efficiency Analysis\n",
    "\n",
    "Evaluate attack efficiency across the entire test dataset with detailed epsilon scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Attack Efficiency Analysis\n",
    "epsilon_range = np.linspace(0.01, 0.3, 20)  # 20 points from 0.01 to 0.3\n",
    "\n",
    "print(\"üöÄ Starting comprehensive attack efficiency analysis...\")\n",
    "print(f\"Testing {len(epsilon_range)} epsilon values: {epsilon_range[0]:.3f} to {epsilon_range[-1]:.3f}\")\n",
    "\n",
    "# Run the analysis\n",
    "efficiency_results = comprehensive_attack_efficiency(\n",
    "    model, test_loader, epsilon_range, device, max_samples=CONFIG['efficiency_samples']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete! Tested on {efficiency_results['sample_count']} samples\")\n",
    "\n",
    "# Create efficiency scaling plots\n",
    "print(\"\\nüìà Creating efficiency scaling visualizations...\")\n",
    "plot_attack_efficiency_scaling(efficiency_results)\n",
    "\n",
    "# Create simple summary plot\n",
    "print(\"\\nüéØ Creating simple efficiency summary plot...\")\n",
    "plot_simple_efficiency_summary(efficiency_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3843300",
   "metadata": {},
   "source": [
    "## üéØ Latent Space Clustering Analysis\n",
    "\n",
    "Analyze how adversarial attacks affect digit clustering in the 2D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce4be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Space Clustering Analysis\n",
    "print(\"üéØ Starting Latent Space Clustering Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Sample digits from test dataset\n",
    "digit_samples = sample_digits_by_class(\n",
    "    test_loader, device, \n",
    "    samples_per_class=CONFIG['clustering_samples'], \n",
    "    max_batches=15\n",
    ")\n",
    "\n",
    "# Analyze clustering under different attacks\n",
    "attack_methods = ['fgsm', 'pgd', 'custom']\n",
    "epsilon_test = 0.15  # Use moderate epsilon for clear visualization\n",
    "clustering_analyses = {}\n",
    "\n",
    "for attack_method in attack_methods:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ANALYZING {attack_method.upper()} ATTACK\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Perform clustering analysis\n",
    "    clustering_results = analyze_latent_clustering_under_attack(\n",
    "        model, digit_samples, attack_method=attack_method, epsilon=epsilon_test\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    clustering_analyses[attack_method] = clustering_results\n",
    "    \n",
    "    # Plot the results\n",
    "    plot_latent_clustering_comparison(clustering_results)\n",
    "\n",
    "# Comparative analysis\n",
    "print(\"\\nüî¨ Creating comparative analysis...\")\n",
    "compare_latent_disruption_across_attacks(clustering_analyses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b82ebd",
   "metadata": {},
   "source": [
    "## üíæ Save Analysis Results\n",
    "\n",
    "Save all analysis results for future reference and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive analysis results\n",
    "print(\"üíæ Saving analysis results...\")\n",
    "\n",
    "# Save different analysis results\n",
    "analysis_files = []\n",
    "\n",
    "# Save attack effectiveness comparison\n",
    "effectiveness_file = save_analysis_results(\n",
    "    comparison_results, model_timestamp, 'attack_effectiveness'\n",
    ")\n",
    "analysis_files.append(effectiveness_file)\n",
    "\n",
    "# Save success rate analysis\n",
    "success_data = {\n",
    "    'config': CONFIG,\n",
    "    'success_rates': {str(k): v for k, v in success_results.items()},\n",
    "    'total_samples_tested': total_tested\n",
    "}\n",
    "success_file = save_analysis_results(\n",
    "    success_data, model_timestamp, 'success_rates'\n",
    ")\n",
    "analysis_files.append(success_file)\n",
    "\n",
    "# Save efficiency analysis\n",
    "efficiency_data = {\n",
    "    'sample_count': efficiency_results['sample_count'],\n",
    "    'epsilon_range': efficiency_results['epsilons'].tolist(),\n",
    "    'attacks': {name: data for name, data in efficiency_results.items() \n",
    "                if name not in ['epsilons', 'sample_count']}\n",
    "}\n",
    "efficiency_file = save_analysis_results(\n",
    "    efficiency_data, model_timestamp, 'attack_efficiency'\n",
    ")\n",
    "analysis_files.append(efficiency_file)\n",
    "\n",
    "# Save clustering analysis\n",
    "clustering_data = {\n",
    "    'epsilon_tested': epsilon_test,\n",
    "    'attacks': {}\n",
    "}\n",
    "\n",
    "for method, results in clustering_analyses.items():\n",
    "    clustering_data['attacks'][method] = {\n",
    "        'attack_info': results['attack_info'],\n",
    "        'centroid_movements': {},\n",
    "        'scatter_changes': {}\n",
    "    }\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    for digit in range(10):\n",
    "        if (digit in results['original']['latent'] and \n",
    "            digit in results['adversarial']['latent']):\n",
    "            \n",
    "            orig_centroid = np.mean(results['original']['latent'][digit], axis=0)\n",
    "            adv_centroid = np.mean(results['adversarial']['latent'][digit], axis=0)\n",
    "            movement = np.linalg.norm(adv_centroid - orig_centroid)\n",
    "            \n",
    "            orig_points = results['original']['latent'][digit]\n",
    "            adv_points = results['adversarial']['latent'][digit]\n",
    "            orig_scatter = np.mean(np.linalg.norm(orig_points - orig_centroid, axis=1))\n",
    "            adv_scatter = np.mean(np.linalg.norm(adv_points - adv_centroid, axis=1))\n",
    "            scatter_change = ((adv_scatter - orig_scatter) / orig_scatter) * 100\n",
    "            \n",
    "            clustering_data['attacks'][method]['centroid_movements'][str(digit)] = float(movement)\n",
    "            clustering_data['attacks'][method]['scatter_changes'][str(digit)] = float(scatter_change)\n",
    "\n",
    "clustering_file = save_analysis_results(\n",
    "    clustering_data, model_timestamp, 'latent_clustering'\n",
    ")\n",
    "analysis_files.append(clustering_file)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE ADVERSARIAL ATTACK ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nüîç Analysis Summary:\")\n",
    "print(f\"   ‚Ä¢ Visual attack analysis with 6-row comparative visualization\")\n",
    "print(f\"   ‚Ä¢ Attack effectiveness comparison across FGSM, PGD, and Custom methods\")\n",
    "print(f\"   ‚Ä¢ Success rate analysis across {len(CONFIG['attack_epsilons'])} epsilon values\")\n",
    "print(f\"   ‚Ä¢ Comprehensive efficiency scaling with {len(epsilon_range)} epsilon points\")\n",
    "print(f\"   ‚Ä¢ Latent space clustering analysis for {len(attack_methods)} attack methods\")\n",
    "print(f\"   ‚Ä¢ Tested on {efficiency_results['sample_count']} samples for efficiency analysis\")\n",
    "\n",
    "print(f\"\\nüíæ Analysis files saved:\")\n",
    "for file in analysis_files:\n",
    "    print(f\"   üìÅ {file}\")\n",
    "\n",
    "print(f\"\\nüîó Model analyzed: {model_timestamp}\")\n",
    "print(\"\\n‚úÖ Complete adversarial vulnerability assessment finished!\")\n",
    "print(\"üõ°Ô∏è Ready for defense strategy development!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
